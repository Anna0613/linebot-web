"""
RAG (Retrieval-Augmented Generation) service.
Implements retrieval over Postgres+pgvector and generation via existing AIAnalysisService.
å„ªåŒ–ç‰ˆæœ¬ï¼šæ·»åŠ æŸ¥è©¢çµæœå¿«å–ä»¥æå‡æ•ˆèƒ½
æ·±åº¦å„ªåŒ–ç‰ˆæœ¬ï¼šæ·»åŠ æ•ˆèƒ½åˆ†æå’Œå„ªåŒ–é¦–æ¬¡æŸ¥è©¢é€Ÿåº¦
"""
from __future__ import annotations

import logging
import time
from typing import Any, Dict, List, Optional, Tuple
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, text as sql_text

from app.models.knowledge import KnowledgeChunk
from app.services.embedding_service import embed_text
from app.services.ai_analysis_service import AIAnalysisService
from app.services.rerank_service import RerankService, HybridRanker
from app.services.hybrid_search_service import HybridSearchService
from app.services.context_manager import global_context_manager, MessageRole
from app.services.rag_cache import get_rag_cache
from app.services.performance_profiler import PerformanceProfiler, profile_async

logger = logging.getLogger(__name__)


class RAGService:
    MIN_SIMILARITY = 0.7  # default cosine similarity threshold
    TOP_K = 5  # default top K

    @staticmethod
    async def retrieve(
        db: AsyncSession,
        bot_id: str,
        query: str,
        *,
        threshold: Optional[float] = None,
        top_k: Optional[int] = None,
        model_name: Optional[str] = None,
        use_cache: bool = True,
        profiler: Optional[PerformanceProfiler] = None
    ) -> List[Tuple[KnowledgeChunk, float]]:
        """
        Retrieve top chunks across project and global scopes using cosine similarity.
        æ”¯æ´æ–°çš„åµŒå…¥æ¨¡å‹å’Œå‘å¾Œç›¸å®¹æ€§ã€‚
        å„ªåŒ–ç‰ˆæœ¬ï¼šæ”¯æ´æŸ¥è©¢çµæœå¿«å–å’Œæ•ˆèƒ½åˆ†æ
        """
        # å‰µå»ºæ•ˆèƒ½åˆ†æå™¨ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if profiler is None:
            profiler = PerformanceProfiler("retrieve")

        # å˜—è©¦å¾å¿«å–ç²å–çµæœ
        if use_cache:
            async with profiler.measure("cache_lookup"):
                cache = get_rag_cache()
                cached_results = cache.get(
                    bot_id=bot_id,
                    query=query,
                    search_type='vector',
                    threshold=threshold,
                    top_k=top_k,
                    model_name=model_name
                )

                if cached_results is not None:
                    logger.info(f"âœ… å¿«å–å‘½ä¸­: {query[:50]}...")
                    # å°‡å­—å…¸è½‰æ›å› KnowledgeChunk å°è±¡
                    items = []
                    for chunk_dict, score in cached_results:
                        kc = KnowledgeChunk()
                        kc.id = chunk_dict['id']
                        kc.document_id = chunk_dict['document_id']
                        kc.bot_id = chunk_dict['bot_id']
                        kc.content = chunk_dict['content']
                        items.append((kc, score))
                    return items

        logger.info(f"ğŸ” å¿«å–æœªå‘½ä¸­ï¼ŒåŸ·è¡Œå®Œæ•´æª¢ç´¢: {query[:50]}...")
        logger.info(f"ğŸ“Š [RAG] æŸ¥è©¢åƒæ•¸ - bot_id: {bot_id}, model: {model_name}, use_cache: {use_cache}")

        # å¿«å–æœªå‘½ä¸­ï¼ŒåŸ·è¡Œå¯¦éš›æŸ¥è©¢
        # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹ç”ŸæˆæŸ¥è©¢åµŒå…¥ï¼ˆembed_text å…§éƒ¨å·²æœ‰å¿«å–ï¼‰
        logger.info(f"ğŸš€ [RAG] é–‹å§‹ç”Ÿæˆ embedding...")
        import time
        embedding_start = time.time()
        async with profiler.measure("embedding_generation", query_length=len(query)):
            emb = await embed_text(query, model_name, use_cache=use_cache)

        embedding_time = (time.time() - embedding_start) * 1000
        logger.info(f"â±ï¸ [RAG] Embedding ç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: {embedding_time:.2f}ms")

        # è½‰æ›ç‚º pgvector æ ¼å¼å­—ä¸²
        embedding_str = "[" + ",".join(map(str, emb)) + "]"

        # pgvector cosine distance: embedding <=> query_embedding
        # distance = 1 - cosine_similarity
        # Filter by similarity >= MIN_SIMILARITY => distance <= (1 - MIN_SIMILARITY)
        th = RAGService.MIN_SIMILARITY if threshold is None else max(0.0, min(1.0, float(threshold)))
        k = RAGService.TOP_K if top_k is None else max(1, int(top_k))
        max_distance = 1.0 - th

        # å–®ä¸€ 768 ç¶­å‘é‡æ¬„ä½ embeddingï¼ˆpgvectorï¼‰
        # åªæª¢ç´¢æœªåˆªé™¤çš„åˆ‡å¡Šå’Œæ–‡ä»¶
        # å„ªåŒ–ï¼šä½¿ç”¨ HNSW ç´¢å¼•åŠ é€ŸæŸ¥è©¢

        async with profiler.measure("database_query", top_k=k, threshold=th):
            # å…ˆè¨­ç½® ef_search åƒæ•¸ï¼ˆéœ€è¦åˆ†é–‹åŸ·è¡Œï¼‰
            try:
                await db.execute(sql_text("SET LOCAL hnsw.ef_search = 100"))
            except Exception as e:
                logger.debug(f"è¨­ç½® hnsw.ef_search å¤±æ•—ï¼ˆå¯èƒ½ä¸æ”¯æ´ï¼‰: {e}")

            sql = sql_text(
                """
                SELECT kc.*,
                       (1 - (kc.embedding <=> CAST(:q AS vector))) AS score
                FROM knowledge_chunks kc
                JOIN knowledge_documents kd ON kc.document_id = kd.id
                WHERE (kc.bot_id = CAST(:bot_id AS UUID) OR kc.bot_id IS NULL)
                  AND kc.deleted_at IS NULL
                  AND kd.deleted_at IS NULL
                  AND (kc.embedding <=> CAST(:q AS vector)) <= :maxd
                ORDER BY (kc.embedding <=> CAST(:q AS vector))
                LIMIT :k
                """
            )

            rows = (await db.execute(sql, {"q": embedding_str, "bot_id": bot_id, "maxd": max_distance, "k": k})).mappings().all()

        async with profiler.measure("result_processing", result_count=len(rows)):
            items: List[Tuple[KnowledgeChunk, float]] = []
            for r in rows:
                # Build transient KnowledgeChunk-like object for return
                kc = KnowledgeChunk()
                kc.id = r["id"]
                kc.document_id = r["document_id"]
                kc.bot_id = r["bot_id"]
                kc.content = r["content"]
                kc.created_at = r["created_at"]
                kc.updated_at = r["updated_at"]
                items.append((kc, float(r["score"])))

        # å­˜å…¥å¿«å–
        if use_cache and items:
            async with profiler.measure("cache_store"):
                cache = get_rag_cache()
                cache.set(
                    bot_id=bot_id,
                    query=query,
                    search_type='vector',
                    results=items,
                    threshold=threshold,
                    top_k=top_k,
                    model_name=model_name
                )

        # æ‰“å°æ•ˆèƒ½æ‘˜è¦
        summary = profiler.get_summary()
        logger.info(f"ğŸ“Š æª¢ç´¢æ•ˆèƒ½: ç¸½è€—æ™‚ {summary['total_time_ms']:.2f}ms")
        for metric in summary['metrics']:
            logger.info(f"  - {metric['name']}: {metric['duration_ms']:.2f}ms ({metric['percentage']:.1f}%)")

        return items

    @staticmethod
    async def retrieve_with_rerank(
        db: AsyncSession,
        bot_id: str,
        query: str,
        *,
        threshold: Optional[float] = None,
        initial_k: int = 20,
        final_k: int = 5,
        rerank_model: Optional[str] = None,
        use_hybrid: bool = True,
        model_name: Optional[str] = None
    ) -> List[Tuple[KnowledgeChunk, float]]:
        """
        å¸¶é‡æ’åºçš„æª¢ç´¢æ–¹æ³•

        Args:
            db: è³‡æ–™åº«æœƒè©±
            bot_id: Bot ID
            query: æŸ¥è©¢æ–‡æœ¬
            threshold: ç›¸ä¼¼åº¦é–€æª»
            initial_k: åˆå§‹æª¢ç´¢æ•¸é‡ï¼ˆé‡æ’åºå‰ï¼‰
            final_k: æœ€çµ‚è¿”å›æ•¸é‡ï¼ˆé‡æ’åºå¾Œï¼‰
            rerank_model: é‡æ’åºæ¨¡å‹åç¨±
            use_hybrid: æ˜¯å¦ä½¿ç”¨æ··åˆæ’åºï¼ˆçµåˆå‘é‡å’Œé‡æ’åºåˆ†æ•¸ï¼‰
            model_name: åµŒå…¥æ¨¡å‹åç¨±

        Returns:
            é‡æ’åºå¾Œçš„çŸ¥è­˜å¡Šåˆ—è¡¨
        """
        # 1. åˆå§‹æª¢ç´¢æ›´å¤šçµæœ
        initial_results = await RAGService.retrieve(
            db, bot_id, query,
            threshold=threshold,
            top_k=initial_k,
            model_name=model_name
        )

        if len(initial_results) <= final_k:
            # å¦‚æœåˆå§‹çµæœä¸å¤šï¼Œç›´æ¥è¿”å›
            return initial_results[:final_k]

        # 2. æº–å‚™é‡æ’åºæ•¸æ“š
        documents = [(kc.content, score) for kc, score in initial_results]

        try:
            if use_hybrid:
                # ä½¿ç”¨æ··åˆé‡æ’åº
                hybrid_results = await HybridRanker.hybrid_rerank(
                    query=query,
                    documents=documents,
                    rerank_model=rerank_model,
                    top_k=final_k
                )

                # é‡æ–°çµ„ç¹”çµæœ
                final_results = []
                for content, combined_score, score_details in hybrid_results:
                    for kc, _ in initial_results:
                        if kc.content == content:
                            # æ·»åŠ åˆ†æ•¸è©³æƒ…åˆ°çŸ¥è­˜å¡Š
                            kc.score_details = score_details
                            final_results.append((kc, combined_score))
                            break

                return final_results

            else:
                # ä½¿ç”¨ç´”é‡æ’åº
                reranked = await RerankService.rerank(
                    query=query,
                    documents=documents,
                    model_name=rerank_model,
                    top_k=final_k
                )

                # é‡æ–°çµ„ç¹”çµæœ
                final_results = []
                for content, rerank_score in reranked:
                    for kc, original_score in initial_results:
                        if kc.content == content:
                            # æ·»åŠ åŸå§‹åˆ†æ•¸è³‡è¨Š
                            kc.original_score = original_score
                            final_results.append((kc, rerank_score))
                            break

                return final_results

        except Exception as e:
            logger.warning(f"é‡æ’åºå¤±æ•—ï¼Œè¿”å›åŸå§‹çµæœ: {e}")
            return initial_results[:final_k]

    @staticmethod
    async def hybrid_search(
        db: AsyncSession,
        bot_id: str,
        query: str,
        *,
        vector_weight: float = 0.7,
        bm25_weight: float = 0.3,
        top_k: int = 5,
        vector_threshold: float = 0.7,
        model_name: Optional[str] = None
    ) -> List[Tuple[KnowledgeChunk, float]]:
        """
        æ··åˆæœå°‹æ–¹æ³•

        Args:
            db: è³‡æ–™åº«æœƒè©±
            bot_id: Bot ID
            query: æŸ¥è©¢æ–‡æœ¬
            vector_weight: å‘é‡æœå°‹æ¬Šé‡
            bm25_weight: BM25 æœå°‹æ¬Šé‡
            top_k: è¿”å›çµæœæ•¸é‡
            vector_threshold: å‘é‡ç›¸ä¼¼åº¦é–€æª»
            model_name: åµŒå…¥æ¨¡å‹åç¨±

        Returns:
            æ··åˆæœå°‹çµæœ
        """
        try:
            hybrid_service = HybridSearchService()
            results = await hybrid_service.hybrid_search(
                db=db,
                bot_id=bot_id,
                query=query,
                vector_weight=vector_weight,
                bm25_weight=bm25_weight,
                top_k=top_k,
                vector_threshold=vector_threshold,
                model_name=model_name
            )

            # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
            return [(chunk, score) for chunk, score, _ in results]

        except Exception as e:
            logger.warning(f"æ··åˆæœå°‹å¤±æ•—ï¼Œå›é€€åˆ°å‘é‡æœå°‹: {e}")
            return await RAGService.retrieve(
                db, bot_id, query,
                threshold=vector_threshold,
                top_k=top_k,
                model_name=model_name
            )

    @staticmethod
    async def get_knowledge_documents_summary(
        db: AsyncSession,
        bot_id: str
    ) -> List[Dict[str, str]]:
        """
        å–å¾—çŸ¥è­˜åº«æ–‡ä»¶åˆ—è¡¨ï¼ˆæª”æ¡ˆåç¨± + AI æ‘˜è¦ï¼‰

        Args:
            db: è³‡æ–™åº« session
            bot_id: Bot ID

        Returns:
            List[Dict]: æ–‡ä»¶åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ åŒ…å« title å’Œ ai_summary
        """
        try:
            from app.models.knowledge import KnowledgeDocument

            # æŸ¥è©¢è©² bot çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆåŒ…å« project å’Œ globalï¼‰ï¼ŒåªæŸ¥è©¢æœªåˆªé™¤çš„æ–‡ä»¶
            stmt = select(KnowledgeDocument).where(
                ((KnowledgeDocument.bot_id == bot_id) | (KnowledgeDocument.bot_id.is_(None))),
                KnowledgeDocument.deleted_at.is_(None)
            ).order_by(KnowledgeDocument.created_at.desc())

            result = await db.execute(stmt)
            documents = result.scalars().all()

            # æ§‹å»ºæ–‡ä»¶åˆ—è¡¨
            doc_list = []
            for doc in documents:
                # åªåŒ…å«æœ‰æ‘˜è¦çš„æ–‡ä»¶
                if doc.ai_summary:
                    doc_list.append({
                        "title": doc.title or doc.original_file_name or "æœªå‘½åæ–‡ä»¶",
                        "summary": doc.ai_summary
                    })

            logger.info(f"å–å¾—çŸ¥è­˜åº«æ–‡ä»¶åˆ—è¡¨: {len(doc_list)} å€‹æ–‡ä»¶ï¼ˆbot_id={bot_id}ï¼‰")
            return doc_list

        except Exception as e:
            logger.error(f"å–å¾—çŸ¥è­˜åº«æ–‡ä»¶åˆ—è¡¨å¤±æ•—: {e}")
            return []

    @staticmethod
    async def _classify_intent(query: str, db: Optional[AsyncSession] = None, bot_id: Optional[str] = None) -> str:
        """
        ä½¿ç”¨ Groq API åˆ¤æ–·ç”¨æˆ¶æ„åœ–ã€‚

        Args:
            query: ç”¨æˆ¶è¨Šæ¯

        Returns:
            "chat" - é–’èŠ
            "query" - è©¢å•å•é¡Œ/æŸ¥è©¢è³‡è¨Š
        """
        try:
            from app.services.groq_service import GroqService
            from app.config import settings
            import re

            # ä½¿ç”¨ llama-3.1-8b-instant æ¨¡å‹é€²è¡Œæ„åœ–åˆ¤æ–·ï¼ˆå¿«é€Ÿä¸”ä¸æœƒç”¢ç”Ÿ reasoningï¼‰
            intent_model = "llama-3.1-8b-instant"

            # å–å¾—çŸ¥è­˜åº«æ–‡ä»¶åˆ—è¡¨ï¼ˆå¦‚æœæœ‰æä¾› db å’Œ bot_idï¼‰
            knowledge_context = ""
            if db and bot_id:
                try:
                    doc_list = await RAGService.get_knowledge_documents_summary(db, bot_id)
                    if doc_list:
                        knowledge_context = "\n\nã€çŸ¥è­˜åº«æ–‡ä»¶åˆ—è¡¨ã€‘\nä»¥ä¸‹æ˜¯ç³»çµ±ä¸­å·²æœ‰çš„çŸ¥è­˜åº«æ–‡ä»¶åŠå…¶æ‘˜è¦ï¼š\n\n"
                        for i, doc in enumerate(doc_list[:10], 1):  # æœ€å¤šé¡¯ç¤º 10 å€‹æ–‡ä»¶
                            knowledge_context += f"{i}. {doc['title']}\n   æ‘˜è¦ï¼š{doc['summary']}\n\n"
                        knowledge_context += "ã€çŸ¥è­˜åº«æ–‡ä»¶åˆ—è¡¨çµæŸã€‘\n"
                        logger.info(f"æ„åœ–åˆ¤æ–·åŠ å…¥ {len(doc_list)} å€‹çŸ¥è­˜åº«æ–‡ä»¶æ‘˜è¦")
                except Exception as e:
                    logger.warning(f"å–å¾—çŸ¥è­˜åº«æ–‡ä»¶åˆ—è¡¨å¤±æ•—ï¼ˆä¸å½±éŸ¿æ„åœ–åˆ¤æ–·ï¼‰: {e}")

            # å„ªåŒ–çš„ç³»çµ±æç¤ºè© - åŠ å…¥çŸ¥è­˜åº«æ–‡ä»¶åˆ—è¡¨åƒè€ƒ
            intent_system_prompt = (
                "ä½ æ˜¯ä¸€å€‹åš´æ ¼çš„æ„åœ–åˆ†é¡å™¨ã€‚ä½ çš„ä»»å‹™æ˜¯åˆ†æç”¨æˆ¶è¨Šæ¯ï¼Œåˆ¤æ–·å…¶æ ¸å¿ƒæ„åœ–ã€‚\n\n"
                "ã€è¼¸å‡ºæ ¼å¼è¦æ±‚ - éå¸¸é‡è¦ã€‘\n"
                "ä½ å¿…é ˆä¸”åƒ…èƒ½å›ç­”ä»¥ä¸‹å…©å€‹è©ä¹‹ä¸€ï¼šchat æˆ– query\n"
                "ä¸è¦åŠ ä¸Šä»»ä½•æ¨™é»ç¬¦è™Ÿã€è§£é‡‹ã€å¼•è™Ÿã€æ‹¬è™Ÿæˆ–å…¶ä»–æ–‡å­—ã€‚\n"
                "åš´æ ¼éµå®ˆæ ¼å¼ï¼Œåªè¼¸å‡ºä¸€å€‹è‹±æ–‡å–®è©ã€‚\n\n"
                "ã€åˆ†é¡åŸå‰‡ã€‘\n"
                "chatï¼šç”¨æˆ¶çš„ä¸»è¦ç›®çš„æ˜¯é€²è¡Œç¤¾äº¤äº’å‹•ã€æƒ…æ„Ÿäº¤æµæˆ–æ—¥å¸¸å°è©±ï¼Œä¸éœ€è¦å…·é«”è³‡è¨Šæˆ–ç­”æ¡ˆã€‚\n"
                "queryï¼šç”¨æˆ¶çš„ä¸»è¦ç›®çš„æ˜¯ç²å–ç‰¹å®šè³‡è¨Šã€å°‹æ±‚è§£ç­”æˆ–éœ€è¦å¯¦è³ªæ€§çš„å›æ‡‰å…§å®¹ã€‚\n\n"
                "ã€çŸ¥è­˜åº«æ–‡ä»¶åˆ—è¡¨åˆ¤æ–·è¦å‰‡ - æœ€é‡è¦ã€‘\n"
                "å¦‚æœç”¨æˆ¶è¨Šæ¯ä¸­åŒ…å«ã€çŸ¥è­˜åº«æ–‡ä»¶åˆ—è¡¨ã€‘å€å¡Šï¼š\n"
                "1. ä»”ç´°é–±è®€åˆ—è¡¨ä¸­æ¯å€‹æ–‡ä»¶çš„ã€Œæª”æ¡ˆåç¨±ã€å’Œã€Œæ‘˜è¦ã€\n"
                "2. åˆ¤æ–·ç”¨æˆ¶å•é¡Œæ˜¯å¦èˆ‡ä»»ä¸€æ–‡ä»¶çš„ä¸»é¡Œæˆ–å…§å®¹ç›¸é—œ\n"
                "3. åˆ¤æ–·æ¨™æº–ï¼š\n"
                "   - å¦‚æœç”¨æˆ¶å•é¡Œæ˜ç¢ºè©¢å•æ–‡ä»¶ä¸­æåˆ°çš„ä¸»é¡Œã€æ¦‚å¿µæˆ–è³‡è¨Š â†’ è¼¸å‡º query\n"
                "   - å¦‚æœç”¨æˆ¶å•é¡Œèˆ‡æ‰€æœ‰æ–‡ä»¶çš„ä¸»é¡Œéƒ½å®Œå…¨ç„¡é—œ â†’ è¼¸å‡º chat\n"
                "   - å¦‚æœç”¨æˆ¶å•é¡Œæ˜¯é–’èŠã€å•å€™ã€æƒ…æ„Ÿè¡¨é”ç­‰ç¤¾äº¤æ€§è³ª â†’ è¼¸å‡º chat\n"
                "4. ç¯„ä¾‹èªªæ˜ï¼š\n"
                "   - æ–‡ä»¶æ‘˜è¦æåˆ°ã€Œç‡Ÿæ¥­æ™‚é–“ã€ï¼Œç”¨æˆ¶å•ã€Œä½ å€‘é€±æœ«æœ‰é–‹å—ã€â†’ queryï¼ˆç›¸é—œï¼‰\n"
                "   - æ–‡ä»¶æ‘˜è¦æåˆ°ã€Œç”¢å“åƒ¹æ ¼ã€ï¼Œç”¨æˆ¶å•ã€Œä»Šå¤©å¤©æ°£çœŸå¥½ã€â†’ chatï¼ˆç„¡é—œï¼‰\n"
                "   - æ–‡ä»¶æ‘˜è¦æåˆ°ã€Œé€€è²¨æ”¿ç­–ã€ï¼Œç”¨æˆ¶å•ã€Œå¦‚ä½•é€€è²¨ã€â†’ queryï¼ˆç›¸é—œï¼‰\n\n"
                "å¦‚æœç”¨æˆ¶è¨Šæ¯ä¸­æ²’æœ‰ã€çŸ¥è­˜åº«æ–‡ä»¶åˆ—è¡¨ã€‘å€å¡Šï¼š\n"
                "- æŒ‰ç…§ä¸€èˆ¬åˆ†é¡åŸå‰‡åˆ¤æ–·ï¼ˆé–’èŠ â†’ chatï¼ŒæŸ¥è©¢è³‡è¨Š â†’ queryï¼‰\n\n"
                "è«‹æ ¹æ“šä»¥ä¸Šè¦å‰‡é€²è¡Œåˆ¤æ–·ï¼Œåªè¼¸å‡º chat æˆ– queryã€‚"
            )

            # æ§‹å»ºç”¨æˆ¶è¨Šæ¯ï¼ˆåŒ…å«çŸ¥è­˜åº«æ–‡ä»¶åˆ—è¡¨ï¼‰
            user_message = query
            if knowledge_context:
                user_message = f"{knowledge_context}\nã€ç”¨æˆ¶å•é¡Œã€‘\n{query}"
                logger.info(
                    f"ğŸ“‹ æ„åœ–åˆ¤æ–· - ç™¼é€çµ¦ AI çš„å®Œæ•´è¨Šæ¯ï¼š\n"
                    f"{'='*60}\n"
                    f"ã€ç³»çµ±æç¤ºè©ã€‘\n{intent_system_prompt}\n"
                    f"{'='*60}\n"
                    f"ã€ç”¨æˆ¶è¨Šæ¯ï¼ˆåŒ…å«çŸ¥è­˜åº«æ–‡ä»¶åˆ—è¡¨ï¼‰ã€‘\n{user_message}\n"
                    f"{'='*60}"
                )
            else:
                logger.info(
                    f"ğŸ“‹ æ„åœ–åˆ¤æ–· - ç™¼é€çµ¦ AI çš„è¨Šæ¯ï¼ˆç„¡çŸ¥è­˜åº«æ–‡ä»¶åˆ—è¡¨ï¼‰ï¼š\n"
                    f"   ç”¨æˆ¶å•é¡Œ: '{query}'"
                )

            # å‘¼å« Groq API
            client = GroqService._get_client(settings.GROQ_API_KEY)

            completion = await client.chat.completions.create(
                model=intent_model,
                messages=[
                    {"role": "system", "content": intent_system_prompt},
                    {"role": "user", "content": user_message}
                ],
                temperature=0.0,  # ä½¿ç”¨ 0 æº«åº¦ä»¥ç²å¾—æœ€ç¢ºå®šçš„åˆ†é¡
                max_tokens=10,    # 10 å€‹ tokens è¶³å¤ è¼¸å‡º chat æˆ– query
                top_p=1.0,        # ä½¿ç”¨ç¢ºå®šæ€§æ¡æ¨£
                stream=False
            )

            # è¨˜éŒ„å®Œæ•´çš„ API å›æ‡‰ä»¥ä¾¿é™¤éŒ¯
            logger.info(
                f"ğŸ” æ„åœ–åˆ¤æ–· API å›æ‡‰ï¼š\n"
                f"   æ¨¡å‹: {intent_model}\n"
                f"   ç”¨æˆ¶å•é¡Œ: '{query}'\n"
                f"   AI å›æ‡‰: {completion.choices[0].message.content if completion.choices else 'None'}"
            )

            # è§£æå›æ‡‰
            if completion.choices and len(completion.choices) > 0:
                raw_response = completion.choices[0].message.content

                # æ¸…ç†å›æ‡‰ï¼šç§»é™¤æ‰€æœ‰ç©ºç™½å­—å…ƒã€æ¨™é»ç¬¦è™Ÿã€å¼•è™Ÿç­‰
                cleaned_intent = re.sub(r'[^\w]', '', raw_response.strip().lower())

                logger.info(
                    f"ğŸ“ æ„åœ–åˆ¤æ–·è§£æéç¨‹ï¼š\n"
                    f"   åŸå§‹å›æ‡‰: '{raw_response}'\n"
                    f"   æ¸…ç†å¾Œ: '{cleaned_intent}'\n"
                    f"   å›æ‡‰é•·åº¦: {len(raw_response)} å­—å…ƒ"
                )

                # åš´æ ¼åŒ¹é…ï¼šå®Œå…¨ç­‰æ–¼ 'chat' æˆ– 'query'
                if cleaned_intent == "chat":
                    logger.info(f"âœ… æ„åœ–åˆ¤æ–·çµæœ: é–’èŠ (åŸå§‹å›æ‡‰: '{raw_response}')")
                    return "chat"
                elif cleaned_intent == "query":
                    logger.info(f"âœ… æ„åœ–åˆ¤æ–·çµæœ: æŸ¥è©¢ (åŸå§‹å›æ‡‰: '{raw_response}')")
                    return "query"
                else:
                    # å®¹éŒ¯è™•ç†ï¼šæª¢æŸ¥æ˜¯å¦åŒ…å«é—œéµè©
                    if "chat" in cleaned_intent:
                        logger.warning(
                            f"âš ï¸ æ„åœ–åˆ¤æ–·åŒ…å« 'chat' ä½†æ ¼å¼ä¸æ¨™æº–\n"
                            f"   åŸå§‹å›æ‡‰: '{raw_response}'\n"
                            f"   æ¸…ç†å¾Œ: '{cleaned_intent}'\n"
                            f"   åˆ¤å®šç‚º: é–’èŠ"
                        )
                        return "chat"
                    elif "query" in cleaned_intent:
                        logger.warning(
                            f"âš ï¸ æ„åœ–åˆ¤æ–·åŒ…å« 'query' ä½†æ ¼å¼ä¸æ¨™æº–\n"
                            f"   åŸå§‹å›æ‡‰: '{raw_response}'\n"
                            f"   æ¸…ç†å¾Œ: '{cleaned_intent}'\n"
                            f"   åˆ¤å®šç‚º: æŸ¥è©¢"
                        )
                        return "query"
                    else:
                        # å®Œå…¨ç„¡æ³•è­˜åˆ¥ï¼Œè¨˜éŒ„è©³ç´°è³‡è¨Šä¸¦é è¨­ç‚ºæŸ¥è©¢
                        logger.warning(
                            f"âŒ æ„åœ–åˆ¤æ–·çµæœä¸æ˜ç¢ºï¼Œç„¡æ³•è­˜åˆ¥ï¼\n"
                            f"   ç”¨æˆ¶è¨Šæ¯: '{query}'\n"
                            f"   åŸå§‹å›æ‡‰: '{raw_response}'\n"
                            f"   æ¸…ç†å¾Œ: '{cleaned_intent}'\n"
                            f"   å›æ‡‰é•·åº¦: {len(raw_response)} å­—å…ƒ\n"
                            f"   å›æ‡‰é¡å‹: {type(raw_response)}\n"
                            f"   å®Œæ•´ completion ç‰©ä»¶: {completion}\n"
                            f"   é è¨­ç‚º: æŸ¥è©¢æ¨¡å¼"
                        )
                        return "query"

            # é è¨­ç‚ºæŸ¥è©¢
            logger.warning(
                f"âš ï¸ æ„åœ–åˆ¤æ–·ç„¡å›æ‡‰\n"
                f"   ç”¨æˆ¶è¨Šæ¯: '{query}'\n"
                f"   å®Œæ•´ completion ç‰©ä»¶: {completion}\n"
                f"   é è¨­ç‚º: æŸ¥è©¢"
            )
            return "query"

        except Exception as e:
            logger.error(
                f"âŒ æ„åœ–åˆ¤æ–·å¤±æ•—\n"
                f"   éŒ¯èª¤è¨Šæ¯: {e}\n"
                f"   ç”¨æˆ¶è¨Šæ¯: '{query}'\n"
                f"   é è¨­ç‚º: æŸ¥è©¢",
                exc_info=True
            )
            # ç™¼ç”ŸéŒ¯èª¤æ™‚é è¨­ç‚ºæŸ¥è©¢ï¼Œç¢ºä¿ä¸æœƒéºæ¼é‡è¦å•é¡Œ
            return "query"

    @staticmethod
    async def generate_answer(
        query: str,
        contexts: List[str],
        *,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        history: Optional[List[dict]] = None,
        system_prompt: Optional[str] = None,
    ) -> str:
        """Use existing AIAnalysisService to generate an answer with given contexts."""
        context_text = "\n\n".join([f"[ç‰‡æ®µ{i+1}]\n{c}" for i, c in enumerate(contexts)])
        result = await AIAnalysisService.ask_ai(
            query,
            context_text=context_text,
            history=history,
            model=model,
            provider=provider,
            system_prompt=(
                system_prompt
                or (
                    "ä½ æ˜¯ LINE èŠå¤©æ©Ÿå™¨äººï¼Œæ­£åœ¨å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚ç³»çµ±å·²æä¾›ç›¸é—œçš„çŸ¥è­˜åº«è³‡æ–™ã€‚\n\n"
                    "å›è¦†è¦æ±‚ï¼š\n"
                    "ãƒ»ç”¨è‡ªå·±çš„è©±æ•´ç†è³‡è¨Šï¼Œä¸è¦ç›´æ¥è¤‡è£½è²¼ä¸ŠåŸæ–‡\n"
                    "ãƒ»ç°¡æ½”æ˜ç¢ºï¼Œç›´æ¥å›ç­”é‡é»ï¼ˆé¿å…å†—é•·çš„é–‹å ´ç™½æˆ–çµå°¾ï¼‰\n"
                    "ãƒ»åˆ†æ®µæ¸…æ¥šï¼Œæ–¹ä¾¿åœ¨ LINE ä¸Šé–±è®€\n"
                    "ãƒ»å¦‚æœè³‡æ–™ä¸è¶³ï¼Œç°¡å–®èªªæ˜å³å¯ï¼Œä¸éœ€è¦éåº¦é“æ­‰\n"
                    "ãƒ»èªæ°£è‡ªç„¶å‹å–„ï¼Œä½†ä¿æŒå°ˆæ¥­æº–ç¢º\n\n"
                    "ã€æ ¼å¼è¦ç¯„ - é‡è¦ï¼ã€‘\n"
                    "ä½ çš„å›è¦†æœƒé¡¯ç¤ºåœ¨ LINE å¡ç‰‡ä¸­ï¼Œè«‹åš´æ ¼éµå®ˆä»¥ä¸‹è¦å‰‡ï¼š\n\n"
                    "âœ— çµ•å°ç¦æ­¢ä½¿ç”¨é€™äº›ç¬¦è™Ÿï¼š\n"
                    "  **æ–‡å­—**ï¼ˆç²—é«”ï¼‰ã€*æ–‡å­—*ï¼ˆæ–œé«”ï¼‰ã€`ä»£ç¢¼`ã€# æ¨™é¡Œã€- åˆ—è¡¨ã€* åˆ—è¡¨ã€> å¼•ç”¨ã€[é€£çµ](ç¶²å€)\n\n"
                    "âœ“ è«‹æ”¹ç”¨é€™äº›æ–¹å¼ï¼š\n"
                    "  ã€æ¨™é¡Œã€‘ç”¨ä¸­æ–‡æ‹¬è™Ÿå¼·èª¿\n"
                    "  ãƒ»é …ç›® ç”¨æ—¥æ–‡ä¸­é»åˆ—èˆ‰\n"
                    "  1. é …ç›® ç”¨æ•¸å­—ç·¨è™Ÿ\n"
                    "  ç›´æ¥æ›è¡Œåˆ†æ®µå³å¯\n\n"
                    "ç¯„ä¾‹ï¼ˆæ­£ç¢ºï¼‰ï¼š\n"
                    "ã€ç‡Ÿæ¥­æ™‚é–“ã€‘\n"
                    "ãƒ»é€±ä¸€è‡³é€±äº”ï¼š9:00-18:00\n"
                    "ãƒ»é€±æœ«ï¼šä¼‘æ¯\n"
                    "\n"
                    "å¦‚æœ‰ç‰¹æ®Šéœ€æ±‚ï¼Œè«‹æå‰è¯ç¹«ã€‚"
                )
            ),
            max_tokens=None,  # ç§»é™¤ç¡¬ç·¨ç¢¼é™åˆ¶ï¼Œè®“ Groq æœå‹™è‡ªå‹•è¨ˆç®—åˆé©çš„ max_tokens
        )
        answer = (result or {}).get("answer", "")
        answer = (answer or "").strip()
        if not answer:
            answer = "æˆ‘åœ¨é€™è£¡ï¼Œè«‹å‘Šè¨´æˆ‘æ‚¨çš„å•é¡Œã€‚"
        return answer

    @staticmethod
    async def answer(
        db: AsyncSession,
        bot_id: str,
        query: str,
        *,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        threshold: Optional[float] = None,
        top_k: Optional[int] = None,
        line_user_id: Optional[str] = None,
        history_messages: Optional[int] = None,
        system_prompt: Optional[str] = None,
        use_rerank: bool = True,
        rerank_model: Optional[str] = None,
        embedding_model: Optional[str] = None,
        use_hybrid: bool = False,
        vector_weight: float = 0.7,
        bm25_weight: float = 0.3,
    ) -> Optional[str]:
        try:
            # ========== æ­¥é©Ÿ 1: æ„åœ–åˆ¤æ–·ï¼ˆåŠ å…¥çŸ¥è­˜åº«æ–‡ä»¶åˆ—è¡¨å„ªåŒ–ï¼‰==========
            logger.info(f"ğŸ” é–‹å§‹æ„åœ–åˆ¤æ–·: {query}")
            intent = await RAGService._classify_intent(query, db=db, bot_id=bot_id)
            logger.info(f"âœ… æ„åœ–åˆ¤æ–·å®Œæˆ: {intent}")

            # æ§‹å»ºå°è©±æ­·å²ï¼ˆå…©ç¨®æƒ…æ³éƒ½éœ€è¦ï¼‰
            hist: Optional[List[dict]] = None
            if line_user_id and (history_messages or 0) > 0:
                try:
                    from app.services.conversation_service import ConversationService
                    conv = await ConversationService.get_or_create_conversation(bot_id, line_user_id)
                    if conv and conv.messages:
                        msgs = conv.messages[-int(history_messages):]
                        hist = []
                        for m in msgs:
                            role = 'assistant' if getattr(m, 'sender_type', '') == 'bot' else 'user'
                            content = ''
                            mc = getattr(m, 'content', None)
                            if isinstance(mc, dict):
                                content = str(mc.get('text') or mc.get('altText') or '')
                            elif isinstance(mc, str):
                                content = mc
                            if content:
                                hist.append({'role': role, 'content': content})
                except Exception as _e:
                    logger.warning(f"æ§‹å»ºæ­·å²å°è©±å¤±æ•—: {_e}")

            # ========== æ­¥é©Ÿ 2: æ ¹æ“šæ„åœ–åˆ†æµè™•ç† ==========
            if intent == "chat":
                # é–’èŠæ¨¡å¼ï¼šè·³é RAG æª¢ç´¢ï¼Œç›´æ¥ä½¿ç”¨ AI ç”Ÿæˆå›è¦†
                logger.info("ğŸ’¬ é–’èŠæ¨¡å¼ï¼šè·³éçŸ¥è­˜åº«æª¢ç´¢ï¼Œç›´æ¥ç”Ÿæˆå›è¦†")

                # ä½¿ç”¨ç©ºçš„ contexts åˆ—è¡¨ï¼Œè®“ AI è‡ªç”±å°è©±
                return await RAGService.generate_answer(
                    query,
                    contexts=[],  # ä¸æä¾›çŸ¥è­˜åº«å…§å®¹
                    provider=provider,
                    model=model,
                    history=hist,
                    system_prompt=system_prompt or (
                        "ä½ æ˜¯ LINE èŠå¤©æ©Ÿå™¨äººï¼Œæ­£åœ¨èˆ‡ç”¨æˆ¶é–’èŠã€‚\n\n"
                        "å›è¦†é¢¨æ ¼ï¼š\n"
                        "ãƒ»ç°¡çŸ­è‡ªç„¶ï¼Œåƒæœ‹å‹èŠå¤©ä¸€æ¨£ï¼ˆ1-2 å¥è©±å³å¯ï¼‰\n"
                        "ãƒ»èªæ°£è¼•é¬†è¦ªåˆ‡ï¼Œä½†ä¸è¦éåº¦ç†±æƒ…æˆ–å†—é•·\n"
                        "ãƒ»å¯ä»¥ç”¨è¡¨æƒ…ç¬¦è™Ÿï¼Œä½†ä¸è¦æ¯å¥éƒ½ç”¨\n"
                        "ãƒ»é¿å…èªªã€Œæœ‰ä»€éº¼éœ€è¦å¹«å¿™ã€ã€ã€Œéš¨æ™‚å‘Šè¨´æˆ‘ã€ç­‰å®¢æœç”¨èª\n"
                        "ãƒ»å°±åƒæ™®é€šæœ‹å‹å›è¨Šæ¯ï¼Œç°¡å–®ã€çœŸèª å°±å¥½\n\n"
                        "ã€æ ¼å¼è¦ç¯„ - é‡è¦ï¼ã€‘\n"
                        "ä½ çš„å›è¦†æœƒé¡¯ç¤ºåœ¨ LINE å¡ç‰‡ä¸­ï¼Œè«‹åš´æ ¼éµå®ˆä»¥ä¸‹è¦å‰‡ï¼š\n\n"
                        "âœ— çµ•å°ç¦æ­¢ä½¿ç”¨é€™äº›ç¬¦è™Ÿï¼š\n"
                        "  **æ–‡å­—**ï¼ˆç²—é«”ï¼‰ã€*æ–‡å­—*ï¼ˆæ–œé«”ï¼‰ã€`ä»£ç¢¼`ã€# æ¨™é¡Œã€- åˆ—è¡¨ã€* åˆ—è¡¨ã€> å¼•ç”¨\n\n"
                        "âœ“ è«‹æ”¹ç”¨é€™äº›æ–¹å¼ï¼š\n"
                        "  ã€æ¨™é¡Œã€‘ç”¨ä¸­æ–‡æ‹¬è™Ÿ\n"
                        "  ãƒ»é …ç›® ç”¨æ—¥æ–‡ä¸­é»\n"
                        "  1. é …ç›® ç”¨æ•¸å­—ç·¨è™Ÿ\n"
                        "  ç›´æ¥æ›è¡Œåˆ†æ®µå³å¯\n\n"
                        "ç¯„ä¾‹ï¼ˆæ­£ç¢ºï¼‰ï¼š\n"
                        "æ—©å®‰ï¼ğŸ˜Š\n"
                        "ç¥ä½ ä»Šå¤©é †åˆ©ï½"
                    ),
                )

            else:  # intent == "query"
                # æŸ¥è©¢æ¨¡å¼ï¼šåŸ·è¡Œå®Œæ•´çš„ RAG æª¢ç´¢æµç¨‹
                logger.info("ğŸ“š æŸ¥è©¢æ¨¡å¼ï¼šåŸ·è¡ŒçŸ¥è­˜åº«æª¢ç´¢")

                # é¸æ“‡æª¢ç´¢æ–¹æ³•
                if use_hybrid:
                    # ä½¿ç”¨æ··åˆæœå°‹
                    items = await RAGService.hybrid_search(
                        db, bot_id, query,
                        vector_weight=vector_weight,
                        bm25_weight=bm25_weight,
                        top_k=top_k or RAGService.TOP_K,
                        vector_threshold=threshold or RAGService.MIN_SIMILARITY,
                        model_name=embedding_model
                    )
                elif use_rerank:
                    # ä½¿ç”¨é‡æ’åº
                    items = await RAGService.retrieve_with_rerank(
                        db, bot_id, query,
                        threshold=threshold,
                        final_k=top_k or RAGService.TOP_K,
                        rerank_model=rerank_model,
                        model_name=embedding_model
                    )
                else:
                    # ä½¿ç”¨åŸºæœ¬å‘é‡æœå°‹
                    items = await RAGService.retrieve(
                        db, bot_id, query,
                        threshold=threshold,
                        top_k=top_k,
                        model_name=embedding_model
                    )

                contexts = [kc.content for kc, _ in items] if items else []
                logger.info(f"ğŸ“– æª¢ç´¢åˆ° {len(contexts)} å€‹çŸ¥è­˜ç‰‡æ®µ")

                return await RAGService.generate_answer(
                    query,
                    contexts,
                    provider=provider,
                    model=model,
                    history=hist,
                    system_prompt=system_prompt,
                )

        except Exception as e:
            logger.error(f"RAG å›ç­”å¤±æ•—: {e}")
            return None

    @staticmethod
    async def answer_with_context_management(
        db: AsyncSession,
        bot_id: str,
        query: str,
        conversation_id: str,
        *,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        threshold: Optional[float] = None,
        top_k: Optional[int] = None,
        system_prompt: Optional[str] = None,
        use_rerank: bool = True,
        use_hybrid: bool = False,
        use_conversation_history: bool = True,
        rerank_model: Optional[str] = None,
        embedding_model: Optional[str] = None,
        vector_weight: float = 0.7,
        bm25_weight: float = 0.3,
    ) -> Optional[str]:
        """
        ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†çš„ RAG å›ç­”æ–¹æ³•

        Args:
            db: è³‡æ–™åº«æœƒè©±
            bot_id: Bot ID
            query: ç”¨æˆ¶æŸ¥è©¢
            conversation_id: å°è©± ID
            provider: AI æä¾›å•†
            model: AI æ¨¡å‹
            threshold: ç›¸ä¼¼åº¦é–€æª»
            top_k: è¿”å›çµæœæ•¸é‡
            system_prompt: ç³»çµ±æç¤ºè©
            use_rerank: æ˜¯å¦ä½¿ç”¨é‡æ’åº
            use_hybrid: æ˜¯å¦ä½¿ç”¨æ··åˆæœå°‹
            use_conversation_history: æ˜¯å¦ä½¿ç”¨å°è©±æ­·å²
            rerank_model: é‡æ’åºæ¨¡å‹
            embedding_model: åµŒå…¥æ¨¡å‹
            vector_weight: å‘é‡æœå°‹æ¬Šé‡
            bm25_weight: BM25 æœå°‹æ¬Šé‡

        Returns:
            AI å›æ‡‰
        """
        try:
            # é¸æ“‡æª¢ç´¢æ–¹æ³•
            if use_hybrid:
                items = await RAGService.hybrid_search(
                    db, bot_id, query,
                    vector_weight=vector_weight,
                    bm25_weight=bm25_weight,
                    top_k=top_k or RAGService.TOP_K,
                    vector_threshold=threshold or RAGService.MIN_SIMILARITY,
                    model_name=embedding_model
                )
            elif use_rerank:
                items = await RAGService.retrieve_with_rerank(
                    db, bot_id, query,
                    threshold=threshold,
                    final_k=top_k or RAGService.TOP_K,
                    rerank_model=rerank_model,
                    model_name=embedding_model
                )
            else:
                items = await RAGService.retrieve(
                    db, bot_id, query,
                    threshold=threshold,
                    top_k=top_k,
                    model_name=embedding_model
                )

            # æ§‹å»ºçŸ¥è­˜åº«ä¸Šä¸‹æ–‡
            contexts = [kc.content for kc, _ in items] if items else []
            context_text = "\n\n".join([f"[ç‰‡æ®µ{i+1}]\n{c}" for i, c in enumerate(contexts)])

            # ä½¿ç”¨ Groq æœå‹™çš„ä¸Šä¸‹æ–‡ç®¡ç†æ–¹æ³•
            if provider == "groq" or not provider:
                from app.services.groq_service import GroqService
                return await GroqService.ask_groq_with_context_management(
                    question=query,
                    context_text=context_text,
                    conversation_id=conversation_id,
                    model=model,
                    system_prompt=system_prompt,
                    use_conversation_history=use_conversation_history
                )
            else:
                # å›é€€åˆ°åŸæœ‰æ–¹æ³•
                return await RAGService.generate_answer(
                    query,
                    contexts,
                    provider=provider,
                    model=model,
                    system_prompt=system_prompt
                )

        except Exception as e:
            logger.error(f"ä¸Šä¸‹æ–‡ç®¡ç† RAG å›ç­”å¤±æ•—: {e}")
            return None

    @staticmethod
    def get_conversation_stats(conversation_id: str) -> Dict[str, Any]:
        """ç²å–å°è©±çµ±è¨ˆè³‡è¨Š"""
        try:
            conversation = global_context_manager.get_conversation(conversation_id)
            return conversation.get_memory_stats()
        except Exception as e:
            logger.error(f"ç²å–å°è©±çµ±è¨ˆå¤±æ•—: {e}")
            return {}

    @staticmethod
    def clear_conversation_memory(conversation_id: str) -> bool:
        """æ¸…é™¤å°è©±è¨˜æ†¶"""
        try:
            conversation = global_context_manager.get_conversation(conversation_id)
            conversation.clear_memory()
            return True
        except Exception as e:
            logger.error(f"æ¸…é™¤å°è©±è¨˜æ†¶å¤±æ•—: {e}")
            return False
