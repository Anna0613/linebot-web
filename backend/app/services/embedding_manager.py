"""
åµŒå…¥æ¨¡å‹ç®¡ç†å™¨
æä¾›å¤šç¨®åµŒå…¥æ¨¡å‹çš„çµ±ä¸€ç®¡ç†å’Œæ‰¹æ¬¡è™•ç†èƒ½åŠ›ã€‚
æ”¯æ´ Gemini APIï¼ˆå„ªå…ˆï¼‰å’Œæœ¬åœ° sentence-transformersï¼ˆå‚™ç”¨ï¼‰ã€‚
"""
from __future__ import annotations

import os
import asyncio
import logging
import hashlib
import time
from functools import lru_cache
from typing import Dict, List, Optional, Any, Tuple, TYPE_CHECKING
import numpy as np
import psutil

# åƒ…åœ¨å‹åˆ¥æª¢æŸ¥æ™‚åŒ¯å…¥ï¼Œé¿å…åŸ·è¡ŒæœŸè§¸ç™¼é‡é‡ç´šç›¸ä¾
if TYPE_CHECKING:  # pragma: no cover
    from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)


class EmbeddingManager:
    """åµŒå…¥æ¨¡å‹ç®¡ç†å™¨ï¼Œæ”¯æ´å¤šç¨®æ¨¡å‹å’Œæ‰¹æ¬¡è™•ç†"""

    _models: Dict[str, "SentenceTransformer"] = {}
    _model_cache_size = 1000  # LRU å¿«å–å¤§å°
    _gemini_client = None  # Gemini API å®¢æˆ¶ç«¯
    _use_gemini = True  # æ˜¯å¦å„ªå…ˆä½¿ç”¨ Gemini API

    # æ”¯æ´çš„åµŒå…¥æ¨¡å‹é…ç½®
    SUPPORTED_MODELS = {
        "gemini-embedding": {
            "name": "models/text-embedding-004",
            "dimensions": 768,
            "max_seq_length": 2048,
            "description": "Google Gemini embedding APIï¼ˆå„ªå…ˆä½¿ç”¨ï¼Œé€Ÿåº¦å¿« 30 å€ï¼‰",
            "performance": "æ¥µå¿«",
            "quality": "é«˜",
            "multilingual": True,
            "type": "api"
        },
        "all-mpnet-base-v2": {
            "name": "sentence-transformers/all-mpnet-base-v2",
            "dimensions": 768,
            "max_seq_length": 384,
            "description": "é«˜å“è³ªæ¨¡å‹ï¼Œæ›´å¥½çš„èªç¾©ç†è§£ï¼ˆå‚™ç”¨ï¼‰",
            "performance": "ä¸­ç­‰",
            "quality": "é«˜",
            "multilingual": False,
            "type": "local"
        },
        "paraphrase-multilingual-mpnet-base-v2": {
            "name": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
            "dimensions": 768,
            "max_seq_length": 128,
            "description": "å¤šèªè¨€é‡‹ç¾©æ¨¡å‹ï¼Œé©åˆä¸­æ–‡è™•ç†ï¼ˆå‚™ç”¨ï¼‰",
            "performance": "ä¸­ç­‰",
            "quality": "é«˜",
            "multilingual": True,
            "type": "local"
        }
    }

    # é è¨­æ¨¡å‹ï¼ˆå„ªå…ˆä½¿ç”¨ Gemini APIï¼‰
    DEFAULT_MODEL = "gemini-embedding"
    FALLBACK_MODEL = "all-mpnet-base-v2"  # å‚™ç”¨æ¨¡å‹

    class _DummyModel:
        """é›¢ç·šæˆ–ä¸‹è¼‰å¤±æ•—æ™‚çš„æ›¿ä»£æ¨¡å‹ï¼Œç”Ÿæˆå¯é‡ç¾çš„å›ºå®šç¶­åº¦å‘é‡"""

        def __init__(self, dim: int) -> None:
            self.dim = dim

        def encode(
            self,
            texts: List[str],
            normalize_embeddings: bool = True,
            convert_to_numpy: bool = True,
            show_progress_bar: bool = False,
            # èˆ‡ sentence-transformers ä»‹é¢å°é½Šï¼Œå¿½ç•¥ä¸æ”¯æ´åƒæ•¸
            batch_size: Optional[int] = None,
            **_: Any,
        ):
            def _embed_one(t: str) -> np.ndarray:
                # ä»¥ MD5 ä½œç‚ºç¨®å­ï¼Œç”¢ç”Ÿå¯é‡ç¾çš„å½éš¨æ©Ÿå‘é‡
                h = hashlib.md5(t.encode("utf-8")).digest()
                seed = int.from_bytes(h[:8], "big", signed=False)
                rng = np.random.default_rng(seed)
                vec = rng.normal(0, 1, size=self.dim).astype(np.float32)
                if normalize_embeddings:
                    n = np.linalg.norm(vec)
                    if n > 0:
                        vec = vec / n
                return vec

            arrs = [_embed_one(x) for x in texts]
            if convert_to_numpy:
                return np.stack(arrs, axis=0)
            return [a.tolist() for a in arrs]
    
    @classmethod
    def _init_gemini_client(cls):
        """åˆå§‹åŒ– Gemini API å®¢æˆ¶ç«¯"""
        logger.info(f"ğŸ”§ [init_gemini] é–‹å§‹åˆå§‹åŒ– Gemini å®¢æˆ¶ç«¯...")
        logger.info(f"ğŸ” [init_gemini] ç•¶å‰ç‹€æ…‹ - _gemini_client: {cls._gemini_client is not None}, _use_gemini: {cls._use_gemini}")

        if cls._gemini_client is not None:
            logger.info(f"âœ… [init_gemini] å®¢æˆ¶ç«¯å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›")
            return cls._gemini_client

        try:
            logger.info(f"ğŸ“¦ [init_gemini] å°å…¥ google.generativeai...")
            import google.generativeai as genai
            logger.info(f"âœ… [init_gemini] google.generativeai å°å…¥æˆåŠŸ")

            logger.info(f"ğŸ”‘ [init_gemini] æª¢æŸ¥ GEMINI_API_KEY...")
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                logger.error(f"âŒ [init_gemini] æœªæ‰¾åˆ° GEMINI_API_KEYï¼Œå°‡ä½¿ç”¨æœ¬åœ°æ¨¡å‹")
                cls._use_gemini = False
                return None

            logger.info(f"âœ… [init_gemini] API Key å·²æ‰¾åˆ°ï¼ˆé•·åº¦: {len(api_key)} å­—å…ƒï¼‰")
            logger.info(f"ğŸ”§ [init_gemini] é…ç½® Gemini API...")
            genai.configure(api_key=api_key)
            cls._gemini_client = genai
            logger.info(f"âœ… [init_gemini] Gemini API å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸ")
            return cls._gemini_client

        except ImportError as e:
            logger.error(f"âŒ [init_gemini] æœªå®‰è£ google-generativeai: {e}")
            cls._use_gemini = False
            return None
        except Exception as e:
            logger.error(f"âŒ [init_gemini] Gemini API åˆå§‹åŒ–å¤±æ•—: {type(e).__name__}: {e}")
            import traceback
            logger.error(f"ğŸ” [init_gemini] å †ç–Šè¿½è¹¤:\n{traceback.format_exc()}")
            cls._use_gemini = False
            return None

    @classmethod
    async def _embed_with_gemini(
        cls,
        text: str,
        task_type: str = "retrieval_query"
    ) -> Optional[List[float]]:
        """ä½¿ç”¨ Gemini API ç”Ÿæˆ embedding"""
        logger.info(f"ğŸ” [Gemini] é–‹å§‹ç”Ÿæˆ embeddingï¼Œæ–‡æœ¬é•·åº¦: {len(text)} å­—å…ƒ")

        if not cls._use_gemini:
            logger.warning(f"âš ï¸ [Gemini] Gemini å·²è¢«ç¦ç”¨ (_use_gemini=False)")
            return None

        logger.info(f"ğŸ”§ [Gemini] åˆå§‹åŒ– Gemini å®¢æˆ¶ç«¯...")
        init_start = time.time()
        client = cls._init_gemini_client()
        init_time = (time.time() - init_start) * 1000
        logger.info(f"â±ï¸ [Gemini] å®¢æˆ¶ç«¯åˆå§‹åŒ–è€—æ™‚: {init_time:.2f}ms")

        if client is None:
            logger.error(f"âŒ [Gemini] å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—")
            return None

        try:
            logger.info(f"ğŸ“¡ [Gemini] æº–å‚™èª¿ç”¨ API...")
            api_start = time.time()

            def _generate():
                logger.info(f"ğŸŒ [Gemini] æ­£åœ¨èª¿ç”¨ embed_content API...")
                call_start = time.time()
                result = client.embed_content(
                    model="models/text-embedding-004",
                    content=text,
                    task_type=task_type
                )
                call_time = (time.time() - call_start) * 1000
                logger.info(f"âœ… [Gemini] API èª¿ç”¨å®Œæˆï¼Œè€—æ™‚: {call_time:.2f}ms")
                return result['embedding']

            # åœ¨ç·šç¨‹æ± ä¸­åŸ·è¡Œä»¥é¿å…é˜»å¡ï¼Œä¸¦åŠ å…¥è¶…æ™‚ä¿è­·
            logger.info(f"ğŸ”„ [Gemini] ä½¿ç”¨ asyncio.to_thread åŸ·è¡Œ (å«è¶…æ™‚ä¿è­·)...")
            thread_start = time.time()
            try:
                embedding = await asyncio.wait_for(asyncio.to_thread(_generate), timeout=8.0)
            except asyncio.TimeoutError:
                logger.warning("â° [Gemini] ç”Ÿæˆè¶…æ™‚ï¼ˆ>8sï¼‰ï¼Œé™ç´šåˆ°æœ¬åœ°æ¨¡å‹")
                return None
            thread_time = (time.time() - thread_start) * 1000

            total_time = (time.time() - api_start) * 1000
            logger.info(f"â±ï¸ [Gemini] asyncio.to_thread è€—æ™‚: {thread_time:.2f}ms")
            logger.info(f"â±ï¸ [Gemini] API ç¸½è€—æ™‚: {total_time:.2f}ms")
            logger.info(f"âœ… [Gemini] Embedding ç”ŸæˆæˆåŠŸï¼Œå‘é‡ç¶­åº¦: {len(embedding)}")

            return embedding

        except Exception as e:
            logger.error(f"âŒ [Gemini] API embedding ç”Ÿæˆå¤±æ•—: {type(e).__name__}: {e}")
            logger.error(f"ğŸ“‹ [Gemini] éŒ¯èª¤è©³æƒ…: {str(e)}")
            import traceback
            logger.error(f"ğŸ” [Gemini] å †ç–Šè¿½è¹¤:\n{traceback.format_exc()}")
            cls._use_gemini = False  # æš«æ™‚ç¦ç”¨ Gemini
            logger.warning(f"âš ï¸ [Gemini] å·²æš«æ™‚ç¦ç”¨ Gemini APIï¼Œå¾ŒçºŒå°‡ä½¿ç”¨æœ¬åœ°æ¨¡å‹")
            return None

    @classmethod
    def get_model_info(cls, model_name: str = None) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹è³‡è¨Š"""
        model_name = model_name or cls.DEFAULT_MODEL
        return cls.SUPPORTED_MODELS.get(model_name, {})
    
    @classmethod
    def list_available_models(cls) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        models = []
        for model_id, config in cls.SUPPORTED_MODELS.items():
            models.append({
                "id": model_id,
                "name": config["name"],
                "dimensions": config["dimensions"],
                "max_seq_length": config["max_seq_length"],
                "description": config["description"],
                "performance": config["performance"],
                "quality": config["quality"],
                "multilingual": config["multilingual"]
            })
        return models
    
    @classmethod
    def get_model(cls, model_name: str = None) -> "SentenceTransformer":
        """ç²å–æˆ–è¼‰å…¥åµŒå…¥æ¨¡å‹"""
        model_name = model_name or cls.DEFAULT_MODEL
        
        if model_name not in cls._models:
            if model_name not in cls.SUPPORTED_MODELS:
                logger.warning(f"ä¸æ”¯æ´çš„æ¨¡å‹: {model_name}ï¼Œä½¿ç”¨é è¨­æ¨¡å‹: {cls.DEFAULT_MODEL}")
                model_name = cls.DEFAULT_MODEL
            
            model_config = cls.SUPPORTED_MODELS[model_name]
            logger.info(f"è¼‰å…¥åµŒå…¥æ¨¡å‹: {model_config['name']}")
            
            try:
                # åœ¨åŒ¯å…¥ transformers/sentence-transformers å‰ï¼Œé¡¯å¼åœç”¨ TensorFlow/Flax å¾Œç«¯ä»¥é¿å…ä¸å¿…è¦çš„ç›¸ä¾å°å…¥
                # æ³¨æ„ï¼šTransformers ä¸»è¦è®€å– USE_TF/USE_FLAXï¼Œæ•…ä¸€ä½µè¨­å®š
                os.environ.setdefault("USE_TF", "0")
                os.environ.setdefault("USE_FLAX", "0")
                os.environ.setdefault("TRANSFORMERS_NO_TF", "1")
                os.environ.setdefault("TRANSFORMERS_NO_FLAX", "1")
                os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
                # é™ä½ TensorFlow ç›¸é—œå™ªéŸ³ï¼ˆè‹¥ç’°å¢ƒä¸­å­˜åœ¨ TFï¼Œä¹Ÿä¸æœƒè¢«è¼‰å…¥ï¼‰
                os.environ.setdefault("TF_ENABLE_ONEDNN_OPTS", "0")

                from sentence_transformers import SentenceTransformer as _SentenceTransformer

                cls._models[model_name] = _SentenceTransformer(model_config["name"])
                logger.info(f"æ¨¡å‹è¼‰å…¥æˆåŠŸ: {model_name}")
            except Exception as e:
                logger.error(f"è¼‰å…¥æ¨¡å‹å¤±æ•—: {model_name}, éŒ¯èª¤: {e}")
                # ä¸‹è¼‰/ç¶²è·¯å¤±æ•—æ™‚çš„å›é€€ç­–ç•¥
                fallback_dim = cls.SUPPORTED_MODELS.get(model_name, {}).get("dimensions", 768)
                # è‹¥ä¸æ˜¯é è¨­æ¨¡å‹ï¼Œå†å˜—è©¦åŠ è¼‰é è¨­æ¨¡å‹
                if model_name != cls.DEFAULT_MODEL:
                    try:
                        logger.info(f"å˜—è©¦è¼‰å…¥é è¨­æ¨¡å‹: {cls.DEFAULT_MODEL}")
                        default_config = cls.SUPPORTED_MODELS[cls.DEFAULT_MODEL]
                        from sentence_transformers import SentenceTransformer as _SentenceTransformer
                        cls._models[cls.DEFAULT_MODEL] = _SentenceTransformer(default_config["name"])  # type: ignore
                        return cls._models[cls.DEFAULT_MODEL]
                    except Exception as e2:
                        logger.warning(f"é è¨­æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œå•Ÿç”¨é›¢ç·šæ›¿ä»£æ¨¡å‹: {e2}")
                        cls._models[model_name] = EmbeddingManager._DummyModel(fallback_dim)  # type: ignore
                else:
                    logger.warning("å•Ÿç”¨é›¢ç·šæ›¿ä»£æ¨¡å‹ï¼ˆç„¡æ³•ä¸‹è¼‰ sentence-transformers æ¬Šé‡ï¼‰")
                    cls._models[model_name] = EmbeddingManager._DummyModel(fallback_dim)  # type: ignore
            
        return cls._models[model_name]
    
    @classmethod
    async def embed_text(
        cls,
        text: str,
        model_name: str = None,
        normalize_embeddings: bool = True
    ) -> List[float]:
        """
        å–®ä¸€æ–‡æœ¬åµŒå…¥ï¼ˆå„ªå…ˆä½¿ç”¨ Gemini APIï¼Œå¤±æ•—å‰‡é™ç´šåˆ°æœ¬åœ°æ¨¡å‹ï¼‰

        Args:
            text: è¦åµŒå…¥çš„æ–‡æœ¬
            model_name: æ¨¡å‹åç¨±ï¼ˆNone å‰‡ä½¿ç”¨é è¨­æ¨¡å‹ï¼‰
            normalize_embeddings: æ˜¯å¦æ¨™æº–åŒ–åµŒå…¥å‘é‡

        Returns:
            List[float]: åµŒå…¥å‘é‡
        """
        overall_start = time.time()
        logger.info(f"ğŸ“ [embed_text] é–‹å§‹è™•ç†ï¼Œæ–‡æœ¬é•·åº¦: {len(text)} å­—å…ƒ")
        logger.info(f"ğŸ”§ [embed_text] è«‹æ±‚æ¨¡å‹: {model_name}, é è¨­æ¨¡å‹: {cls.DEFAULT_MODEL}, Gemini å•Ÿç”¨: {cls._use_gemini}")

        model_name = model_name or cls.DEFAULT_MODEL
        logger.info(f"ğŸ¯ [embed_text] æœ€çµ‚ä½¿ç”¨æ¨¡å‹: {model_name}")

        # å¦‚æœæŒ‡å®šä½¿ç”¨ Gemini æˆ–ä½¿ç”¨é è¨­æ¨¡å‹ï¼ˆGeminiï¼‰
        if model_name == "gemini-embedding" or (model_name == cls.DEFAULT_MODEL and cls._use_gemini):
            logger.info(f"ğŸš€ [embed_text] å˜—è©¦ä½¿ç”¨ Gemini API...")
            start_time = time.time()

            # å˜—è©¦ä½¿ç”¨ Gemini API
            embedding = await cls._embed_with_gemini(text, task_type="retrieval_query")

            if embedding is not None:
                elapsed_ms = (time.time() - start_time) * 1000
                total_ms = (time.time() - overall_start) * 1000
                logger.info(f"âœ… [embed_text] Gemini embedding ç”ŸæˆæˆåŠŸ (Gemini: {elapsed_ms:.2f}ms, ç¸½è¨ˆ: {total_ms:.2f}ms)")
                return embedding

            # Gemini å¤±æ•—ï¼Œé™ç´šåˆ°æœ¬åœ°æ¨¡å‹
            logger.warning(f"âš ï¸ [embed_text] Gemini API ä¸å¯ç”¨ï¼Œé™ç´šåˆ°æœ¬åœ°æ¨¡å‹: {cls.FALLBACK_MODEL}")
            model_name = cls.FALLBACK_MODEL

        # ä½¿ç”¨æœ¬åœ° sentence-transformers æ¨¡å‹
        logger.info(f"ğŸ”§ [embed_text] ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {model_name}")

        def _encode():
            encode_start = time.time()
            logger.info(f"ğŸ“¦ [embed_text] è¼‰å…¥æ¨¡å‹...")
            model = cls.get_model(model_name)
            load_time = (time.time() - encode_start) * 1000
            logger.info(f"â±ï¸ [embed_text] æ¨¡å‹è¼‰å…¥è€—æ™‚: {load_time:.2f}ms")

            logger.info(f"ğŸ”„ [embed_text] é–‹å§‹ encode...")
            encode_start = time.time()
            embedding = model.encode(
                [text],
                normalize_embeddings=normalize_embeddings,
                convert_to_numpy=True,
                show_progress_bar=False,
                batch_size=1
            )[0]
            encode_time = (time.time() - encode_start) * 1000
            logger.info(f"â±ï¸ [embed_text] encode è€—æ™‚: {encode_time:.2f}ms")

            return embedding.tolist()

        start_time = time.time()
        logger.info(f"ğŸ”„ [embed_text] ä½¿ç”¨ asyncio.to_thread åŸ·è¡Œæœ¬åœ°æ¨¡å‹...")
        result = await asyncio.to_thread(_encode)
        elapsed_ms = (time.time() - start_time) * 1000
        total_ms = (time.time() - overall_start) * 1000
        logger.info(f"âœ… [embed_text] æœ¬åœ°æ¨¡å‹ embedding ç”ŸæˆæˆåŠŸ (æœ¬åœ°: {elapsed_ms:.2f}ms, ç¸½è¨ˆ: {total_ms:.2f}ms)")

        return result
    
    @classmethod
    @lru_cache(maxsize=1000)
    def embed_text_cached(
        cls, 
        text: str, 
        model_name: str = None,
        normalize_embeddings: bool = True
    ) -> List[float]:
        """å–®ä¸€æ–‡æœ¬åµŒå…¥ï¼ˆå¸¶å¿«å–ï¼‰"""
        model_name = model_name or cls.DEFAULT_MODEL
        model = cls.get_model(model_name)
        embedding = model.encode(
            [text], 
            normalize_embeddings=normalize_embeddings,
            convert_to_numpy=True
        )[0]
        return embedding.tolist()
    
    @classmethod
    async def _get_system_load(cls) -> float:
        """ç²å–ç³»çµ±è² è¼‰æŒ‡æ¨™"""
        try:
            cpu_percent = psutil.cpu_percent(interval=0.1)
            memory_percent = psutil.virtual_memory().percent
            # å– CPU å’Œè¨˜æ†¶é«”ä½¿ç”¨ç‡çš„æœ€å¤§å€¼ä½œç‚ºè² è¼‰æŒ‡æ¨™
            return max(cpu_percent, memory_percent) / 100.0
        except Exception as e:
            logger.warning(f"ç²å–ç³»çµ±è² è¼‰å¤±æ•—: {e}")
            return 0.5  # é è¨­ä¸­ç­‰è² è¼‰

    @classmethod
    def _calculate_optimal_batch_size(
        cls,
        texts: List[str],
        base_batch_size: int = 32,
        min_batch_size: int = 8,
        max_batch_size: int = 128,
        system_load: float = 0.5
    ) -> int:
        """
        æ ¹æ“šæ–‡æœ¬ç‰¹å¾µå’Œç³»çµ±è² è¼‰è¨ˆç®—æœ€ä½³æ‰¹æ¬¡å¤§å°

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            base_batch_size: åŸºç¤æ‰¹æ¬¡å¤§å°
            min_batch_size: æœ€å°æ‰¹æ¬¡å¤§å°
            max_batch_size: æœ€å¤§æ‰¹æ¬¡å¤§å°
            system_load: ç³»çµ±è² è¼‰ (0.0-1.0)

        Returns:
            int: æœ€ä½³æ‰¹æ¬¡å¤§å°
        """
        if not texts:
            return base_batch_size

        # è¨ˆç®—å¹³å‡æ–‡æœ¬é•·åº¦
        avg_text_length = sum(len(text) for text in texts) / len(texts)

        # æ ¹æ“šæ–‡æœ¬é•·åº¦èª¿æ•´æ‰¹æ¬¡å¤§å°
        if avg_text_length > 2000:  # é•·æ–‡æœ¬
            length_factor = 0.5
        elif avg_text_length > 1000:  # ä¸­ç­‰æ–‡æœ¬
            length_factor = 0.75
        elif avg_text_length < 200:  # çŸ­æ–‡æœ¬
            length_factor = 1.5
        else:  # æ­£å¸¸æ–‡æœ¬
            length_factor = 1.0

        # æ ¹æ“šç³»çµ±è² è¼‰èª¿æ•´
        if system_load > 0.8:  # é«˜è² è¼‰
            load_factor = 0.5
        elif system_load > 0.6:  # ä¸­é«˜è² è¼‰
            load_factor = 0.75
        elif system_load < 0.3:  # ä½è² è¼‰
            load_factor = 1.5
        else:  # æ­£å¸¸è² è¼‰
            load_factor = 1.0

        # è¨ˆç®—æœ€ä½³æ‰¹æ¬¡å¤§å°
        optimal_size = int(base_batch_size * length_factor * load_factor)

        # é™åˆ¶åœ¨åˆç†ç¯„åœå…§
        optimal_size = max(min_batch_size, min(max_batch_size, optimal_size))

        logger.debug(
            f"æ‰¹æ¬¡å¤§å°è¨ˆç®—: å¹³å‡æ–‡æœ¬é•·åº¦={avg_text_length:.0f}, "
            f"ç³»çµ±è² è¼‰={system_load:.2f}, æœ€ä½³æ‰¹æ¬¡å¤§å°={optimal_size}"
        )

        return optimal_size

    @classmethod
    async def embed_texts_adaptive_batch(
        cls,
        texts: List[str],
        model_name: str = None,
        base_batch_size: int = 32,
        min_batch_size: int = 8,
        max_batch_size: int = 128,
        normalize_embeddings: bool = True,
        show_progress: bool = False
    ) -> List[List[float]]:
        """
        è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°çš„å‘é‡åŒ–è™•ç†

        Args:
            texts: è¦åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨
            model_name: æ¨¡å‹åç¨±
            base_batch_size: åŸºç¤æ‰¹æ¬¡å¤§å°
            min_batch_size: æœ€å°æ‰¹æ¬¡å¤§å°
            max_batch_size: æœ€å¤§æ‰¹æ¬¡å¤§å°
            normalize_embeddings: æ˜¯å¦æ¨™æº–åŒ–åµŒå…¥å‘é‡
            show_progress: æ˜¯å¦é¡¯ç¤ºé€²åº¦

        Returns:
            List[List[float]]: åµŒå…¥å‘é‡åˆ—è¡¨
        """
        model_name = model_name or cls.DEFAULT_MODEL

        if not texts:
            return []

        # ç²å–ç³»çµ±è² è¼‰
        system_load = await cls._get_system_load()

        # è¨ˆç®—æœ€ä½³æ‰¹æ¬¡å¤§å°
        optimal_batch_size = cls._calculate_optimal_batch_size(
            texts, base_batch_size, min_batch_size, max_batch_size, system_load
        )

        logger.info(
            f"é–‹å§‹è‡ªé©æ‡‰æ‰¹æ¬¡å‘é‡åŒ–: {len(texts)} å€‹æ–‡æœ¬, "
            f"æ‰¹æ¬¡å¤§å°={optimal_batch_size}, ç³»çµ±è² è¼‰={system_load:.2f}"
        )

        def _process_batch(batch_texts: List[str]) -> List[List[float]]:
            model = cls.get_model(model_name)
            embeddings = model.encode(
                batch_texts,
                normalize_embeddings=normalize_embeddings,
                convert_to_numpy=True,
                show_progress_bar=show_progress and len(batch_texts) > 10
            )
            return [emb.tolist() for emb in embeddings]

        # åˆ†æ‰¹è™•ç†
        all_embeddings = []
        total_batches = (len(texts) - 1) // optimal_batch_size + 1

        for i in range(0, len(texts), optimal_batch_size):
            batch = texts[i:i + optimal_batch_size]
            batch_num = i // optimal_batch_size + 1

            logger.debug(f"è™•ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} å€‹æ–‡æœ¬)")

            try:
                batch_embeddings = await asyncio.to_thread(_process_batch, batch)
                all_embeddings.extend(batch_embeddings)
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡ {batch_num} è™•ç†å¤±æ•—: {e}")
                # å¦‚æœæ‰¹æ¬¡è™•ç†å¤±æ•—ï¼Œå˜—è©¦å–®å€‹è™•ç†
                for text in batch:
                    try:
                        single_embedding = await cls.embed_text(text, model_name)
                        all_embeddings.append(single_embedding)
                    except Exception as single_e:
                        logger.error(f"å–®å€‹æ–‡æœ¬è™•ç†å¤±æ•—: {single_e}")
                        # ä½¿ç”¨é›¶å‘é‡ä½œç‚ºå‚™é¸
                        dimensions = cls.get_embedding_dimensions(model_name)
                        all_embeddings.append([0.0] * dimensions)

        logger.info(f"è‡ªé©æ‡‰æ‰¹æ¬¡å‘é‡åŒ–å®Œæˆ: {len(all_embeddings)} å€‹å‘é‡")
        return all_embeddings

    @classmethod
    async def embed_texts_batch(
        cls,
        texts: List[str],
        model_name: str = None,
        batch_size: int = 32,
        normalize_embeddings: bool = True,
        show_progress: bool = False
    ) -> List[List[float]]:
        """
        æ‰¹æ¬¡ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆå„ªå…ˆä½¿ç”¨ Gemini APIï¼Œå¤±æ•—å‰‡é™ç´šåˆ°æœ¬åœ°æ¨¡å‹ï¼‰

        Args:
            texts: è¦åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨
            model_name: æ¨¡å‹åç¨±
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆåƒ…ç”¨æ–¼æœ¬åœ°æ¨¡å‹ï¼‰
            normalize_embeddings: æ˜¯å¦æ¨™æº–åŒ–åµŒå…¥å‘é‡
            show_progress: æ˜¯å¦é¡¯ç¤ºé€²åº¦

        Returns:
            List[List[float]]: åµŒå…¥å‘é‡åˆ—è¡¨
        """
        model_name = model_name or cls.DEFAULT_MODEL

        if not texts:
            return []

        # å¦‚æœæŒ‡å®šä½¿ç”¨ Gemini æˆ–ä½¿ç”¨é è¨­æ¨¡å‹ï¼ˆGeminiï¼‰
        if model_name == "gemini-embedding" or (model_name == cls.DEFAULT_MODEL and cls._use_gemini):
            start_time = time.time()

            # å˜—è©¦ä½¿ç”¨ Gemini APIï¼ˆé€å€‹è™•ç†ï¼Œå› ç‚º Gemini æ²’æœ‰æ‰¹æ¬¡ APIï¼‰
            all_embeddings = []
            for i, text in enumerate(texts):
                embedding = await cls._embed_with_gemini(text, task_type="retrieval_document")

                if embedding is None:
                    # Gemini å¤±æ•—ï¼Œé™ç´šåˆ°æœ¬åœ°æ¨¡å‹è™•ç†å‰©é¤˜æ–‡æœ¬
                    logger.info(f"âš ï¸ Gemini API åœ¨ç¬¬ {i+1}/{len(texts)} å€‹æ–‡æœ¬å¤±æ•—ï¼Œé™ç´šåˆ°æœ¬åœ°æ¨¡å‹")
                    model_name = cls.FALLBACK_MODEL
                    break

                all_embeddings.append(embedding)

                if show_progress and (i + 1) % 10 == 0:
                    logger.info(f"Gemini embedding é€²åº¦: {i+1}/{len(texts)}")

            # å¦‚æœå…¨éƒ¨æˆåŠŸ
            if len(all_embeddings) == len(texts):
                elapsed_ms = (time.time() - start_time) * 1000
                logger.info(f"âœ… Gemini æ‰¹æ¬¡ embedding å®Œæˆ: {len(texts)} å€‹æ–‡æœ¬ ({elapsed_ms:.2f}ms)")
                return all_embeddings

            # è™•ç†å‰©é¤˜æ–‡æœ¬ï¼ˆå¾å¤±æ•—çš„ä½ç½®é–‹å§‹ï¼‰
            remaining_texts = texts[len(all_embeddings):]
            logger.info(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹è™•ç†å‰©é¤˜ {len(remaining_texts)} å€‹æ–‡æœ¬")
        else:
            remaining_texts = texts
            all_embeddings = []

        # ä½¿ç”¨æœ¬åœ° sentence-transformers æ¨¡å‹
        def _process_batch(batch_texts: List[str]) -> List[List[float]]:
            model = cls.get_model(model_name)
            embeddings = model.encode(
                batch_texts,
                normalize_embeddings=normalize_embeddings,
                convert_to_numpy=True,
                show_progress_bar=show_progress
            )
            return [emb.tolist() for emb in embeddings]

        # åˆ†æ‰¹è™•ç†å‰©é¤˜æ–‡æœ¬
        for i in range(0, len(remaining_texts), batch_size):
            batch = remaining_texts[i:i + batch_size]
            logger.debug(f"è™•ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(remaining_texts)-1)//batch_size + 1}")
            batch_embeddings = await asyncio.to_thread(_process_batch, batch)
            all_embeddings.extend(batch_embeddings)

        return all_embeddings
    
    @classmethod
    def calculate_similarity(
        cls,
        embedding1: List[float],
        embedding2: List[float],
        method: str = "cosine"
    ) -> float:
        """è¨ˆç®—å…©å€‹åµŒå…¥å‘é‡çš„ç›¸ä¼¼åº¦"""
        emb1 = np.array(embedding1)
        emb2 = np.array(embedding2)
        
        if method == "cosine":
            # é¤˜å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(emb1, emb2)
            norm1 = np.linalg.norm(emb1)
            norm2 = np.linalg.norm(emb2)
            if norm1 == 0 or norm2 == 0:
                return 0.0
            return float(dot_product / (norm1 * norm2))
        
        elif method == "euclidean":
            # æ­å¹¾é‡Œå¾—è·é›¢ï¼ˆè½‰æ›ç‚ºç›¸ä¼¼åº¦ï¼‰
            distance = np.linalg.norm(emb1 - emb2)
            return float(1.0 / (1.0 + distance))
        
        elif method == "dot_product":
            # é»ç©
            return float(np.dot(emb1, emb2))
        
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„ç›¸ä¼¼åº¦è¨ˆç®—æ–¹æ³•: {method}")
    
    @classmethod
    def get_embedding_dimensions(cls, model_name: str = None) -> int:
        """ç²å–æ¨¡å‹çš„åµŒå…¥ç¶­åº¦"""
        model_name = model_name or cls.DEFAULT_MODEL
        model_config = cls.SUPPORTED_MODELS.get(model_name)
        if model_config:
            return model_config["dimensions"]
        return 768  # é è¨­ç¶­åº¦
    
    @classmethod
    def clear_cache(cls):
        """æ¸…é™¤å¿«å–"""
        cls.embed_text_cached.cache_clear()
        logger.info("åµŒå…¥å¿«å–å·²æ¸…é™¤")
    
    @classmethod
    def get_cache_info(cls) -> Dict[str, Any]:
        """ç²å–å¿«å–è³‡è¨Š"""
        cache_info = cls.embed_text_cached.cache_info()
        return {
            "hits": cache_info.hits,
            "misses": cache_info.misses,
            "maxsize": cache_info.maxsize,
            "currsize": cache_info.currsize,
            "hit_rate": cache_info.hits / (cache_info.hits + cache_info.misses) if (cache_info.hits + cache_info.misses) > 0 else 0.0
        }

    @classmethod
    def get_embedding_status(cls) -> Dict[str, Any]:
        """ç²å– embedding æœå‹™ç‹€æ…‹"""
        return {
            "gemini_enabled": cls._use_gemini,
            "gemini_available": cls._gemini_client is not None,
            "default_model": cls.DEFAULT_MODEL,
            "fallback_model": cls.FALLBACK_MODEL,
            "api_key_configured": bool(os.getenv("GEMINI_API_KEY"))
        }

    @classmethod
    def enable_gemini(cls, enable: bool = True):
        """å•Ÿç”¨æˆ–ç¦ç”¨ Gemini API"""
        cls._use_gemini = enable
        if enable:
            cls._init_gemini_client()
        logger.info(f"Gemini API {'å•Ÿç”¨' if enable else 'ç¦ç”¨'}")

# æ³¨æ„ï¼šå‘å¾Œç›¸å®¹çš„å‡½æ•¸å·²ç§»è‡³ embedding_service.py
# è«‹å¾ app.services.embedding_service å°å…¥ embed_text å’Œ embed_texts
