#!/usr/bin/env python3
"""
è³‡æ–™åº«æ¶æ§‹ç”Ÿæˆå™¨
ç”¨æ–¼ç”Ÿæˆè³‡æ–™åº«æ¶æ§‹å ±å‘Šå’Œæ–‡æª”
"""
import os
import sys
from typing import Dict, List, Any
from datetime import datetime
from sqlalchemy import MetaData, Table, Column, text
from sqlalchemy.engine import Engine

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
current_dir = os.path.dirname(os.path.abspath(__file__))
scripts_dir = os.path.dirname(current_dir)
backend_dir = os.path.dirname(scripts_dir)
sys.path.insert(0, backend_dir)

from app.config import settings
from app.database import engine


def get_database_schema() -> Dict[str, Any]:
    """
    ç²å–è³‡æ–™åº«æ¶æ§‹ä¿¡æ¯
    
    Returns:
        Dict[str, Any]: åŒ…å«æ‰€æœ‰è¡¨æ ¼æ¶æ§‹ä¿¡æ¯çš„å­—å…¸
    """
    # ä½¿ç”¨å¾ database æ¨¡çµ„å°å…¥çš„ engine
    metadata = MetaData()
    
    try:
        # åå°„ç¾æœ‰çš„è³‡æ–™åº«æ¶æ§‹
        metadata.reflect(bind=engine)
        
        schema_info = {
            "database_url": settings.DATABASE_URL.replace(settings.DATABASE_PASSWORD, "***"),
            "generated_at": datetime.now().isoformat(),
            "tables": {}
        }
        
        for table_name, table in metadata.tables.items():
            table_info = {
                "columns": [],
                "primary_keys": [],
                "foreign_keys": [],
                "indexes": []
            }
            
            # æ”¶é›†æ¬„ä½ä¿¡æ¯
            for column in table.columns:
                column_info = {
                    "name": column.name,
                    "type": str(column.type),
                    "nullable": column.nullable,
                    "default": str(column.default) if column.default else None,
                    "autoincrement": getattr(column, 'autoincrement', False)
                }
                table_info["columns"].append(column_info)
            
            # æ”¶é›†ä¸»éµ
            for pk in table.primary_key.columns:
                table_info["primary_keys"].append(pk.name)
            
            # æ”¶é›†å¤–éµ
            for fk in table.foreign_keys:
                fk_info = {
                    "column": fk.parent.name,
                    "references": f"{fk.column.table.name}.{fk.column.name}"
                }
                table_info["foreign_keys"].append(fk_info)
            
            # æ”¶é›†ç´¢å¼•
            for index in table.indexes:
                index_info = {
                    "name": index.name,
                    "columns": [col.name for col in index.columns],
                    "unique": index.unique
                }
                table_info["indexes"].append(index_info)
            
            schema_info["tables"][table_name] = table_info
        
        return schema_info
        
    except Exception as e:
        return {
            "error": f"ç²å–è³‡æ–™åº«æ¶æ§‹å¤±æ•—: {str(e)}",
            "generated_at": datetime.now().isoformat(),
            "tables": {}
        }


def format_schema_report(schema_info: Dict[str, Any]) -> str:
    """
    æ ¼å¼åŒ–æ¶æ§‹å ±å‘Šç‚º Markdown æ ¼å¼
    
    Args:
        schema_info: å¾ get_database_schema() ç²å–çš„æ¶æ§‹ä¿¡æ¯
    
    Returns:
        str: Markdown æ ¼å¼çš„æ¶æ§‹å ±å‘Š
    """
    report = []
    
    # æ¨™é¡Œå’ŒåŸºæœ¬ä¿¡æ¯
    report.append("# è³‡æ–™åº«æ¶æ§‹å ±å‘Š")
    report.append("")
    report.append(f"**ç”Ÿæˆæ™‚é–“**: {schema_info.get('generated_at', 'N/A')}")
    
    if 'database_url' in schema_info:
        report.append(f"**è³‡æ–™åº«é€£ç·š**: {schema_info['database_url']}")
    
    if 'error' in schema_info:
        report.append("")
        report.append("## âŒ éŒ¯èª¤")
        report.append(f"{schema_info['error']}")
        return "\n".join(report)
    
    report.append("")
    report.append(f"**è¡¨æ ¼æ•¸é‡**: {len(schema_info['tables'])}")
    report.append("")
    
    # ç›®éŒ„
    report.append("## ğŸ“‘ ç›®éŒ„")
    report.append("")
    for table_name in sorted(schema_info['tables'].keys()):
        report.append(f"- [{table_name}](#{table_name.replace('_', '-')})")
    report.append("")
    
    # è¡¨æ ¼è©³ç´°ä¿¡æ¯
    report.append("## ğŸ“Š è¡¨æ ¼è©³ç´°ä¿¡æ¯")
    report.append("")
    
    for table_name, table_info in sorted(schema_info['tables'].items()):
        report.append(f"### {table_name}")
        report.append("")
        
        # æ¬„ä½ä¿¡æ¯
        if table_info['columns']:
            report.append("#### æ¬„ä½")
            report.append("")
            report.append("| æ¬„ä½å | é¡å‹ | å¯ç©ºå€¼ | é è¨­å€¼ | è‡ªå‹•éå¢ |")
            report.append("|--------|------|--------|--------|----------|")
            
            for col in table_info['columns']:
                nullable = "âœ…" if col['nullable'] else "âŒ"
                autoincrement = "âœ…" if col['autoincrement'] else "âŒ"
                default = col['default'] or "-"
                report.append(f"| {col['name']} | {col['type']} | {nullable} | {default} | {autoincrement} |")
            report.append("")
        
        # ä¸»éµ
        if table_info['primary_keys']:
            report.append("#### ä¸»éµ")
            report.append("")
            for pk in table_info['primary_keys']:
                report.append(f"- `{pk}`")
            report.append("")
        
        # å¤–éµ
        if table_info['foreign_keys']:
            report.append("#### å¤–éµ")
            report.append("")
            for fk in table_info['foreign_keys']:
                report.append(f"- `{fk['column']}` â†’ `{fk['references']}`")
            report.append("")
        
        # ç´¢å¼•
        if table_info['indexes']:
            report.append("#### ç´¢å¼•")
            report.append("")
            report.append("| ç´¢å¼•å | æ¬„ä½ | å”¯ä¸€æ€§ |")
            report.append("|--------|------|--------|")
            
            for idx in table_info['indexes']:
                unique = "âœ…" if idx['unique'] else "âŒ"
                columns = ", ".join(idx['columns'])
                report.append(f"| {idx['name']} | {columns} | {unique} |")
            report.append("")
        
        report.append("---")
        report.append("")
    
    # çµ±è¨ˆä¿¡æ¯
    report.append("## ğŸ“ˆ çµ±è¨ˆä¿¡æ¯")
    report.append("")
    
    total_columns = sum(len(table_info['columns']) for table_info in schema_info['tables'].values())
    total_indexes = sum(len(table_info['indexes']) for table_info in schema_info['tables'].values())
    total_foreign_keys = sum(len(table_info['foreign_keys']) for table_info in schema_info['tables'].values())
    
    report.append(f"- **ç¸½è¡¨æ ¼æ•¸**: {len(schema_info['tables'])}")
    report.append(f"- **ç¸½æ¬„ä½æ•¸**: {total_columns}")
    report.append(f"- **ç¸½ç´¢å¼•æ•¸**: {total_indexes}")
    report.append(f"- **ç¸½å¤–éµæ•¸**: {total_foreign_keys}")
    report.append("")
    
    # è¡¨æ ¼é—œè¯åœ– (Mermaid)
    if schema_info['tables']:
        report.append("## ğŸ”— è¡¨æ ¼é—œè¯åœ–")
        report.append("")
        report.append("```mermaid")
        report.append("erDiagram")
        
        # å®šç¾©è¡¨æ ¼
        for table_name, table_info in schema_info['tables'].items():
            # ç°¡åŒ–è¡¨æ ¼å®šç¾©ï¼Œåªé¡¯ç¤ºä¸»è¦æ¬„ä½
            key_columns = []
            for col in table_info['columns'][:5]:  # åªé¡¯ç¤ºå‰5å€‹æ¬„ä½
                col_type = col['type'].split('(')[0]  # ç§»é™¤é¡å‹åƒæ•¸
                key_columns.append(f"    {col_type} {col['name']}")
            
            if len(table_info['columns']) > 5:
                key_columns.append("    ... more_columns")
            
            report.append(f"    {table_name.upper()} {{")
            report.extend(key_columns)
            report.append("    }")
        
        # å®šç¾©é—œè¯
        for table_name, table_info in schema_info['tables'].items():
            for fk in table_info['foreign_keys']:
                ref_table = fk['references'].split('.')[0]
                report.append(f"    {table_name.upper()} ||--o{{ {ref_table.upper()} : {fk['column']}")
        
        report.append("```")
        report.append("")
    
    report.append("---")
    report.append("")
    report.append("*æ­¤å ±å‘Šç”± LineBot-Web è³‡æ–™åº«æ¶æ§‹ç”Ÿæˆå™¨è‡ªå‹•ç”¢ç”Ÿ*")
    
    return "\n".join(report)


def generate_migration_sql(schema_info: Dict[str, Any]) -> str:
    """
    æ ¹æ“šæ¶æ§‹ä¿¡æ¯ç”Ÿæˆå»ºè¡¨ SQL
    
    Args:
        schema_info: è³‡æ–™åº«æ¶æ§‹ä¿¡æ¯
    
    Returns:
        str: SQL å»ºè¡¨èªå¥
    """
    sql_statements = []
    
    sql_statements.append("-- è³‡æ–™åº«æ¶æ§‹ SQL")
    sql_statements.append(f"-- ç”Ÿæˆæ™‚é–“: {schema_info.get('generated_at', 'N/A')}")
    sql_statements.append("")
    
    for table_name, table_info in schema_info['tables'].items():
        sql_statements.append(f"-- è¡¨æ ¼: {table_name}")
        sql_statements.append(f"CREATE TABLE IF NOT EXISTS {table_name} (")
        
        column_definitions = []
        for col in table_info['columns']:
            col_def = f"    {col['name']} {col['type']}"
            
            if not col['nullable']:
                col_def += " NOT NULL"
            
            if col['default']:
                col_def += f" DEFAULT {col['default']}"
            
            column_definitions.append(col_def)
        
        # æ·»åŠ ä¸»éµç´„æŸ
        if table_info['primary_keys']:
            pk_columns = ", ".join(table_info['primary_keys'])
            column_definitions.append(f"    PRIMARY KEY ({pk_columns})")
        
        sql_statements.append(",\n".join(column_definitions))
        sql_statements.append(");")
        sql_statements.append("")
        
        # æ·»åŠ ç´¢å¼•
        for idx in table_info['indexes']:
            if not idx['unique']:  # ä¸»éµå’Œå”¯ä¸€ç´„æŸæœƒè‡ªå‹•å‰µå»ºç´¢å¼•
                columns = ", ".join(idx['columns'])
                sql_statements.append(f"CREATE INDEX IF NOT EXISTS {idx['name']} ON {table_name} ({columns});")
        
        sql_statements.append("")
    
    return "\n".join(sql_statements)


if __name__ == "__main__":
    """å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è³‡æ–™åº«æ¶æ§‹ç”Ÿæˆå™¨")
    parser.add_argument(
        "--format",
        choices=["markdown", "sql", "json"],
        default="markdown",
        help="è¼¸å‡ºæ ¼å¼"
    )
    parser.add_argument(
        "--output",
        help="è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼ˆé è¨­è¼¸å‡ºåˆ°æ§åˆ¶å°ï¼‰"
    )
    
    args = parser.parse_args()
    
    # ç²å–æ¶æ§‹ä¿¡æ¯
    schema_info = get_database_schema()
    
    # æ ¼å¼åŒ–è¼¸å‡º
    if args.format == "markdown":
        output = format_schema_report(schema_info)
    elif args.format == "sql":
        output = generate_migration_sql(schema_info)
    elif args.format == "json":
        import json
        output = json.dumps(schema_info, indent=2, ensure_ascii=False)
    
    # è¼¸å‡ºçµæœ
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(output)
        print(f"âœ… è¼¸å‡ºå·²å„²å­˜è‡³: {args.output}")
    else:
        print(output)